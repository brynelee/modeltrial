{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivector Retriever\n",
    "\n",
    "It can often be beneficial to store multiple vectors per document. There are multiple use cases where this is beneficial. LangChain has a base MultiVectorRetriever which makes querying this type of setup easy. A lot of the complexity lies in how to create the multiple vectors per document. This notebook covers some of the common ways to create those vectors and use the MultiVectorRetriever.\n",
    "\n",
    "The methods to create multiple vectors per document include:\n",
    "\n",
    "    Smaller chunks: split a document into smaller chunks, and embed those (this is ParentDocumentRetriever).\n",
    "    Summary: create a summary for each document, embed that along with (or instead of) the document.\n",
    "    Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document.\n",
    "\n",
    "Note that this also enables another method of adding embeddings - manually. This is great because you can explicitly add questions or queries that should lead to a document being recovered, giving you more control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name = \"moka-ai/m3e-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    TextLoader(\"../../docs/paul_graham_essay.txt\"),\n",
    "    TextLoader(\"../../docs/state_of_the_union.txt\"),\n",
    "]\n",
    "docs = []\n",
    "for l in loaders:\n",
    "    docs.extend(l.load())\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
    "docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# xiaodong added for shorten processing time\n",
    "docs = docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Computers were expensive in those days and it took me years of nagging before I convinced my father to buy one, a TRS-80, in about 1980. The gold standard then was the Apple II, but a TRS-80 was good enough. This was when I really started programming. I wrote simple games, a program to predict how high my model rockets would fly, and a word processor that my father used to write at least one book. There was only room in memory for about 2 pages of text, so he'd write 2 pages at a time and then print them out, but it was a lot better than a typewriter.\", metadata={'source': '../../docs/paul_graham_essay.txt'})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smaller chunks\n",
    "\n",
    "Often times it can be useful to retrieve larger chunks of information, but embed smaller chunks. This allows for embeddings to capture the semantic meaning as closely as possible, but for as much context as possible to be passed downstream. Note that this is what the ParentDocumentRetriever does. Here we show what is going on under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=embedding_model\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "import uuid\n",
    "\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The splitter to use to create smaller chunks\n",
    "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_docs = []\n",
    "for i, doc in enumerate(docs):\n",
    "    _id = doc_ids[i]\n",
    "    _sub_docs = child_text_splitter.split_documents([doc])\n",
    "    for _doc in _sub_docs:\n",
    "        _doc.metadata[id_key] = _id\n",
    "    sub_docs.extend(_sub_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.vectorstore.add_documents(sub_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"good enough. This was when I really started programming. I wrote simple games, a program to predict how high my model rockets would fly, and a word processor that my father used to write at least one book. There was only room in memory for about 2 pages of text, so he'd write 2 pages at a time and\", metadata={'doc_id': '66eb8e17-e49e-4cff-9058-63ce1aec124f', 'source': '../../docs/paul_graham_essay.txt'})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorstore alone retrieves the small chunks\n",
    "retriever.vectorstore.similarity_search(\"word processor\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "557"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retriever returns larger chunks\n",
    "len(retriever.get_relevant_documents(\"word processor\")[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default search type the retriever performs on the vector database is a similarity search. LangChain Vector Stores also support searching via Max Marginal Relevance so if you want this instead you can just set the search_type property as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "557"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers.multi_vector import SearchType\n",
    "\n",
    "retriever.search_type = SearchType.mmr\n",
    "\n",
    "len(retriever.get_relevant_documents(\"word processor\")[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Oftentimes a summary may be able to distill more accurately what a chunk is about, leading to better retrieval. Here we show how to create summaries, and then embed those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.document import Document\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "openai_api_base_address = \"http://192.168.3.84:20000/v1\"\n",
    "chat_model = ChatOpenAI(openai_api_key = \"aaabbbcccdddeeefffedddsfasdfasdf\", \n",
    "    openai_api_base = openai_api_base_address,\n",
    "    model_name = \"chatglm3-6b-32k\",\n",
    "    max_retries=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='What I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.', metadata={'source': '../../docs/paul_graham_essay.txt'})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'在上大学之前，我主要在写作和编程方面工作过。我写的不是论文，而是当时认为是“入门级”的短篇小说。我的小说很糟糕，几乎没有情节，只有角色强烈的感情，我觉得这使得它们具有深度。我第一次尝试编写程序是在9年级，也就是13或14岁的时候，当时我们学校区使用IBM 1401进行数据处理。这所学校区的1401位于我们初中学校的地下室，我的朋友Rich Draves和我得到了使用它的许可。那像一个小型詹姆斯·邦德 villain的巢穴，有各种外星般的机器，包括CPU、磁盘驱动器、打印机和读卡器，都放在一个抬高的地板上，在明亮的荧光灯下。'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = chain.invoke(docs[0])\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=embedding_model)\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We can also add the original chunks to the vectorstore if we so want\n",
    "# for i, doc in enumerate(docs):\n",
    "#     doc.metadata[id_key] = doc_ids[i]\n",
    "# retriever.vectorstore.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"word processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='这篇文档描述了早期Fortran语言的使用方式。当时，用户需要将程序打印在卡片上，然后将卡片放入读卡机中，并按按钮将程序加载到内存中并运行。通常情况下，运行的结果是在极为嘈杂的打印机上打印出一些东西。', metadata={'doc_id': '83ca368b-1652-4d06-a391-4a18ce5d862a'})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(\"word processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothetical Queries\n",
    "An LLM can also be used to generate a list of hypothetical questions that could be asked of a particular document. These questions can then be embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\": \"hypothetical_questions\",\n",
    "        \"description\": \"Generate hypothetical questions\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"questions\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\"type\": \"string\"},\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"questions\"],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n",
    "\n",
    "chat_model = ChatOpenAI(openai_api_key = \"aaabbbcccdddeeefffedddsfasdfasdf\", \n",
    "    openai_api_base = openai_api_base_address,\n",
    "    model_name = \"chatglm3-6b-32k\",\n",
    "    max_retries=0)\n",
    "\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    # Only asking for 3 hypothetical questions, but this could be adjusted\n",
    "    | ChatPromptTemplate.from_template(\n",
    "        \"Generate a list of 3 hypothetical questions that the below document could be used to answer:\\n\\n{doc}\"\n",
    "    )\n",
    "    | chat_model.bind(\n",
    "        functions=functions, function_call={\"name\": \"hypothetical_questions\"}\n",
    "    )\n",
    "    | JsonKeyOutputFunctionsParser(key_name=\"questions\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Could not parse function call: 'function_call'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain\\output_parsers\\openai_functions.py:73\u001b[0m, in \u001b[0;36mJsonOutputFunctionsParser.parse_result\u001b[1;34m(self, result, partial)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m     function_call \u001b[39m=\u001b[39m message\u001b[39m.\u001b[39;49madditional_kwargs[\u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[0;32m     74\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'function_call'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32md:\\Gitroot\\modeltrial\\LangChainStudy\\retriever\\multivector_retriever.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Gitroot/modeltrial/LangChainStudy/retriever/multivector_retriever.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m chain\u001b[39m.\u001b[39;49minvoke(docs[\u001b[39m0\u001b[39;49m])\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\base.py:1470\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   1468\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1469\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[1;32m-> 1470\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[0;32m   1471\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[0;32m   1472\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   1473\u001b[0m             patch_config(\n\u001b[0;32m   1474\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1475\u001b[0m             ),\n\u001b[0;32m   1476\u001b[0m         )\n\u001b[0;32m   1477\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[0;32m   1478\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\output_parsers\\base.py:170\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[0;32m    167\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Union[\u001b[39mstr\u001b[39m, BaseMessage], config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    168\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39minput\u001b[39m, BaseMessage):\n\u001b[1;32m--> 170\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_with_config(\n\u001b[0;32m    171\u001b[0m             \u001b[39mlambda\u001b[39;49;00m inner_input: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse_result(\n\u001b[0;32m    172\u001b[0m                 [ChatGeneration(message\u001b[39m=\u001b[39;49minner_input)]\n\u001b[0;32m    173\u001b[0m             ),\n\u001b[0;32m    174\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[0;32m    175\u001b[0m             config,\n\u001b[0;32m    176\u001b[0m             run_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mparser\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    177\u001b[0m         )\n\u001b[0;32m    178\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[0;32m    180\u001b[0m             \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_result([Generation(text\u001b[39m=\u001b[39minner_input)]),\n\u001b[0;32m    181\u001b[0m             \u001b[39minput\u001b[39m,\n\u001b[0;32m    182\u001b[0m             config,\n\u001b[0;32m    183\u001b[0m             run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    184\u001b[0m         )\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\base.py:885\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m    878\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    879\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    880\u001b[0m     \u001b[39minput\u001b[39m,\n\u001b[0;32m    881\u001b[0m     run_type\u001b[39m=\u001b[39mrun_type,\n\u001b[0;32m    882\u001b[0m     name\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_name\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    883\u001b[0m )\n\u001b[0;32m    884\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 885\u001b[0m     output \u001b[39m=\u001b[39m call_func_with_variable_args(\n\u001b[0;32m    886\u001b[0m         func, \u001b[39minput\u001b[39m, config, run_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    887\u001b[0m     )\n\u001b[0;32m    888\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    889\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\config.py:308\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[39mif\u001b[39;00m run_manager \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    307\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[1;32m--> 308\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\output_parsers\\base.py:171\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[1;34m(inner_input)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[0;32m    167\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Union[\u001b[39mstr\u001b[39m, BaseMessage], config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    168\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39minput\u001b[39m, BaseMessage):\n\u001b[0;32m    170\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[1;32m--> 171\u001b[0m             \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse_result(\n\u001b[0;32m    172\u001b[0m                 [ChatGeneration(message\u001b[39m=\u001b[39;49minner_input)]\n\u001b[0;32m    173\u001b[0m             ),\n\u001b[0;32m    174\u001b[0m             \u001b[39minput\u001b[39m,\n\u001b[0;32m    175\u001b[0m             config,\n\u001b[0;32m    176\u001b[0m             run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    177\u001b[0m         )\n\u001b[0;32m    178\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[0;32m    180\u001b[0m             \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_result([Generation(text\u001b[39m=\u001b[39minner_input)]),\n\u001b[0;32m    181\u001b[0m             \u001b[39minput\u001b[39m,\n\u001b[0;32m    182\u001b[0m             config,\n\u001b[0;32m    183\u001b[0m             run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    184\u001b[0m         )\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain\\output_parsers\\openai_functions.py:130\u001b[0m, in \u001b[0;36mJsonKeyOutputFunctionsParser.parse_result\u001b[1;34m(self, result, partial)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_result\u001b[39m(\u001b[39mself\u001b[39m, result: List[Generation], \u001b[39m*\u001b[39m, partial: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m--> 130\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mparse_result(result, partial\u001b[39m=\u001b[39;49mpartial)\n\u001b[0;32m    131\u001b[0m     \u001b[39mif\u001b[39;00m partial \u001b[39mand\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\langchain\\lib\\site-packages\\langchain\\output_parsers\\openai_functions.py:78\u001b[0m, in \u001b[0;36mJsonOutputFunctionsParser.parse_result\u001b[1;34m(self, result, partial)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m         \u001b[39mraise\u001b[39;00m OutputParserException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not parse function call: \u001b[39m\u001b[39m{\u001b[39;00mexc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     79\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[39mif\u001b[39;00m partial:\n",
      "\u001b[1;31mOutputParserException\u001b[0m: Could not parse function call: 'function_call'"
     ]
    }
   ],
   "source": [
    "chain.invoke(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothetical_questions = chain.batch(docs, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"hypo-questions\", embedding_function=embedding_model\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_docs = []\n",
    "for i, question_list in enumerate(hypothetical_questions):\n",
    "    question_docs.extend(\n",
    "        [Document(page_content=s, metadata={id_key: doc_ids[i]}) for s in question_list]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.vectorstore.add_documents(question_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"word processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(\"word processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(retrieved_docs[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
