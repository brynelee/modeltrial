{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why use LCEL\n",
    "\n",
    "We recommend reading the LCEL Get started section first.\n",
    "\n",
    "LCEL makes it easy to build complex chains from basic components. It does this by providing: 1. A unified interface: Every LCEL object implements the Runnable interface, which defines a common set of invocation methods (invoke, batch, stream, ainvoke, …). This makes it possible for chains of LCEL objects to also automatically support these invocations. That is, every chain of LCEL objects is itself an LCEL object. 2. Composition primitives: LCEL provides a number of primitives that make it easy to compose chains, parallelize components, add fallbacks, dynamically configure chain internal, and more.\n",
    "\n",
    "To better understand the value of LCEL, it’s helpful to see it in action and think about how we might recreate similar functionality without it. In this walkthrough we’ll do just that with our basic example from the get started section. We’ll take our simple prompt + model chain, which under the hood already defines a lot of functionality, and see what it would take to recreate all of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "\n",
    "# model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "from custom_llms import (\n",
    "    ZhipuAIEmbeddings,\n",
    "    Zhipuai_LLM,\n",
    "    load_api\n",
    ")\n",
    "api_key = load_api()\n",
    "model = Zhipuai_LLM(zhipuai_api_key=api_key)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke\n",
    "\n",
    "In the simplest case, we just want to pass in a topic string and get back a joke string:\n",
    "\n",
    "Without LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a short joke about cats\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='Why do cats always land on their feet? Because they have nine lives and still have to prove it! 🐱😅')]], llm_output=None, run=[RunInfo(run_id=UUID('749f61c4-3562-43fa-a96d-6f693bbac857'))])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "prompt_template = \"Tell me a short joke about {topic}\"\n",
    "# client = openai.OpenAI()\n",
    "\n",
    "def call_chat_model(messages: List[dict]) -> str:\n",
    "    response = model.generate(messages)\n",
    "    return response\n",
    "\n",
    "def invoke_chain(topic: str) -> str:\n",
    "    prompt_value = prompt_template.format(topic=topic)\n",
    "    print(prompt_value)\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt_value}]\n",
    "    return call_chat_model(messages)\n",
    "\n",
    "invoke_chain(\"cats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LECL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why don't scientists trust atoms? Because they make up everything!\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "# model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream\n",
    "\n",
    "If we want to stream results instead, we’ll need to change our function:\n",
    "Without LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms? Because they make up everything!"
     ]
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "\n",
    "def stream_chat_model(messages: List[dict]) -> Iterator[str]:\n",
    "    stream = model.generate(messages, stream=True)\n",
    "    return stream.generations[0][0].text\n",
    "\"\"\"     for response in stream:\n",
    "        content = response\n",
    "        if content is not None:\n",
    "            yield content \"\"\"\n",
    "\n",
    "def stream_chain(topic: str) -> Iterator[str]:\n",
    "    prompt_value = prompt.format(topic=topic)\n",
    "    return stream_chat_model([{\"role\": \"user\", \"content\": prompt_value}])\n",
    "\n",
    "# stream_chain(\"ice cream\")\n",
    "\n",
    "for chunk in stream_chain(\"ice cream\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LECL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code\n",
      "msg\n",
      "data\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "\"\"\" for chunk in chain.stream(\"ice cream\"):\n",
    "    print(chunk, end=\"\", flush=True) \"\"\"\n",
    "for chunk in chain.stream(\"ice cream\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Why don't scientists trust atoms? Because they make up everything!\",\n",
       " \"Why don't scientists trust spaghetti? Because it's always pulling pasta tricks! 😄\",\n",
       " 'Why did the dumpling win an award?因为它成功地“包裹”了最佳口感！']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why don't scientists trust atoms? Because they make up everything!\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain.ainvoke(\"ice cream\")\n",
    "# chain.abatch_invoke([\"ice cream\", \"ice cream\", \"ice cream\"])\n",
    "response =await chain.ainvoke(\"ice cream\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['猫：喵，给你讲一个关于猫的笑话。\\\\n\\\\n有一天，一只猫走进了一家汽车制造厂，对工程师说：“我想买一辆会爬树的汽车。”\\\\n\\\\n工程师惊讶地问：“会爬树的汽车？你确定吗？”\\\\n\\\\n猫回答：“当然，不然我买它干嘛？”\\\\n\\\\n工程师想了想，说：“那你等着，我给你定制一辆。”\\\\n\\\\n一周后，工程师把一辆带有很多利爪的汽车交给了猫。猫非常高兴，付完钱后就开着新车离开了。\\\\n\\\\n第二天，猫回到工厂，满脸愤怒地对工程师说：“骗子！你说这车会爬树，结果它只会挖洞！”\\\\n\\\\n工程师一脸无辜地回答：“但它不是给你定制了一辆带有很多利爪的汽车吗？”\\\\n\\\\n猫无语地走了，心想：“谁说猫一定会爬树呢？”\\\\n\\\\n哈哈，喵，你觉得这个笑话有趣吗？',\n",
       " '一个人走进了宠物店，问老板：“这只狗多少钱？”老板回答：“这是 Wirehaired Pointing Griffon，它需要一点时间来适应新环境，所以价格是其他的狗的两倍。”那个人问：“为什么要这么贵？”老板回答：“你没看见它后面的那个牌子吗？写着‘友情提醒：主人是个瞎子。’”',\n",
       " '一个人问：“猪是怎么上树的？” 另一个人回答：“变成烤猪蹄就上去了。”\\\\n\\\\n（注：这个笑话是在调侃猪一般来说不会上树，但经过烹饪加工后的烤猪蹄却可以挂在树上。）']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([\"猫\",\"狗\",\"猪\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelize steps\n",
    "\n",
    "RunnableParallel (aka. RunnableMap) makes it easy to execute multiple Runnables in parallel, and to return the output of these Runnables as a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': \"Why don't scientists trust atoms?\\\\n\\\\nBecause they make up everything!\",\n",
       " 'poem': 'Python, sleek and divine,\\\\nSlithering through codeine.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joke_chain = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\n",
    "poem_chain = (\n",
    "    ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\") | model\n",
    ")\n",
    "\n",
    "from langchain_core.runnables.base import RunnableParallel\n",
    "map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)\n",
    "\n",
    "map_chain.invoke({\"topic\": \"python\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
