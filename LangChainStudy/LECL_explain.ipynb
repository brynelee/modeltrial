{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why use LCEL\n",
    "\n",
    "We recommend reading the LCEL Get started section first.\n",
    "\n",
    "LCEL makes it easy to build complex chains from basic components. It does this by providing: 1. A unified interface: Every LCEL object implements the Runnable interface, which defines a common set of invocation methods (invoke, batch, stream, ainvoke, â€¦). This makes it possible for chains of LCEL objects to also automatically support these invocations. That is, every chain of LCEL objects is itself an LCEL object. 2. Composition primitives: LCEL provides a number of primitives that make it easy to compose chains, parallelize components, add fallbacks, dynamically configure chain internal, and more.\n",
    "\n",
    "To better understand the value of LCEL, itâ€™s helpful to see it in action and think about how we might recreate similar functionality without it. In this walkthrough weâ€™ll do just that with our basic example from the get started section. Weâ€™ll take our simple prompt + model chain, which under the hood already defines a lot of functionality, and see what it would take to recreate all of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "\n",
    "# model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "from custom_llms import (\n",
    "    ZhipuAIEmbeddings,\n",
    "    Zhipuai_LLM,\n",
    "    load_api\n",
    ")\n",
    "api_key = load_api()\n",
    "model = Zhipuai_LLM(zhipuai_api_key=api_key)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke\n",
    "\n",
    "In the simplest case, we just want to pass in a topic string and get back a joke string:\n",
    "\n",
    "Without LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a short joke about cats\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='Why do cats always land on their feet? Because they have nine lives and still have to prove it! ğŸ±ğŸ˜…')]], llm_output=None, run=[RunInfo(run_id=UUID('749f61c4-3562-43fa-a96d-6f693bbac857'))])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "prompt_template = \"Tell me a short joke about {topic}\"\n",
    "# client = openai.OpenAI()\n",
    "\n",
    "def call_chat_model(messages: List[dict]) -> str:\n",
    "    response = model.generate(messages)\n",
    "    return response\n",
    "\n",
    "def invoke_chain(topic: str) -> str:\n",
    "    prompt_value = prompt_template.format(topic=topic)\n",
    "    print(prompt_value)\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt_value}]\n",
    "    return call_chat_model(messages)\n",
    "\n",
    "invoke_chain(\"cats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LECL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why don't scientists trust atoms? Because they make up everything!\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "# model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream\n",
    "\n",
    "If we want to stream results instead, weâ€™ll need to change our function:\n",
    "Without LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms? Because they make up everything!"
     ]
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "\n",
    "def stream_chat_model(messages: List[dict]) -> Iterator[str]:\n",
    "    stream = model.generate(messages, stream=True)\n",
    "    return stream.generations[0][0].text\n",
    "\"\"\"     for response in stream:\n",
    "        content = response\n",
    "        if content is not None:\n",
    "            yield content \"\"\"\n",
    "\n",
    "def stream_chain(topic: str) -> Iterator[str]:\n",
    "    prompt_value = prompt.format(topic=topic)\n",
    "    return stream_chat_model([{\"role\": \"user\", \"content\": prompt_value}])\n",
    "\n",
    "# stream_chain(\"ice cream\")\n",
    "\n",
    "for chunk in stream_chain(\"ice cream\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LECL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code\n",
      "msg\n",
      "data\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "\"\"\" for chunk in chain.stream(\"ice cream\"):\n",
    "    print(chunk, end=\"\", flush=True) \"\"\"\n",
    "for chunk in chain.stream(\"ice cream\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Why don't scientists trust atoms? Because they make up everything!\",\n",
       " \"Why don't scientists trust spaghetti? Because it's always pulling pasta tricks! ğŸ˜„\",\n",
       " 'Why did the dumpling win an award?å› ä¸ºå®ƒæˆåŠŸåœ°â€œåŒ…è£¹â€äº†æœ€ä½³å£æ„Ÿï¼']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why don't scientists trust atoms? Because they make up everything!\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain.ainvoke(\"ice cream\")\n",
    "# chain.abatch_invoke([\"ice cream\", \"ice cream\", \"ice cream\"])\n",
    "response =await chain.ainvoke(\"ice cream\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['çŒ«ï¼šå–µï¼Œç»™ä½ è®²ä¸€ä¸ªå…³äºçŒ«çš„ç¬‘è¯ã€‚\\\\n\\\\næœ‰ä¸€å¤©ï¼Œä¸€åªçŒ«èµ°è¿›äº†ä¸€å®¶æ±½è½¦åˆ¶é€ å‚ï¼Œå¯¹å·¥ç¨‹å¸ˆè¯´ï¼šâ€œæˆ‘æƒ³ä¹°ä¸€è¾†ä¼šçˆ¬æ ‘çš„æ±½è½¦ã€‚â€\\\\n\\\\nå·¥ç¨‹å¸ˆæƒŠè®¶åœ°é—®ï¼šâ€œä¼šçˆ¬æ ‘çš„æ±½è½¦ï¼Ÿä½ ç¡®å®šå—ï¼Ÿâ€\\\\n\\\\nçŒ«å›ç­”ï¼šâ€œå½“ç„¶ï¼Œä¸ç„¶æˆ‘ä¹°å®ƒå¹²å˜›ï¼Ÿâ€\\\\n\\\\nå·¥ç¨‹å¸ˆæƒ³äº†æƒ³ï¼Œè¯´ï¼šâ€œé‚£ä½ ç­‰ç€ï¼Œæˆ‘ç»™ä½ å®šåˆ¶ä¸€è¾†ã€‚â€\\\\n\\\\nä¸€å‘¨åï¼Œå·¥ç¨‹å¸ˆæŠŠä¸€è¾†å¸¦æœ‰å¾ˆå¤šåˆ©çˆªçš„æ±½è½¦äº¤ç»™äº†çŒ«ã€‚çŒ«éå¸¸é«˜å…´ï¼Œä»˜å®Œé’±åå°±å¼€ç€æ–°è½¦ç¦»å¼€äº†ã€‚\\\\n\\\\nç¬¬äºŒå¤©ï¼ŒçŒ«å›åˆ°å·¥å‚ï¼Œæ»¡è„¸æ„¤æ€’åœ°å¯¹å·¥ç¨‹å¸ˆè¯´ï¼šâ€œéª—å­ï¼ä½ è¯´è¿™è½¦ä¼šçˆ¬æ ‘ï¼Œç»“æœå®ƒåªä¼šæŒ–æ´ï¼â€\\\\n\\\\nå·¥ç¨‹å¸ˆä¸€è„¸æ— è¾œåœ°å›ç­”ï¼šâ€œä½†å®ƒä¸æ˜¯ç»™ä½ å®šåˆ¶äº†ä¸€è¾†å¸¦æœ‰å¾ˆå¤šåˆ©çˆªçš„æ±½è½¦å—ï¼Ÿâ€\\\\n\\\\nçŒ«æ— è¯­åœ°èµ°äº†ï¼Œå¿ƒæƒ³ï¼šâ€œè°è¯´çŒ«ä¸€å®šä¼šçˆ¬æ ‘å‘¢ï¼Ÿâ€\\\\n\\\\nå“ˆå“ˆï¼Œå–µï¼Œä½ è§‰å¾—è¿™ä¸ªç¬‘è¯æœ‰è¶£å—ï¼Ÿ',\n",
       " 'ä¸€ä¸ªäººèµ°è¿›äº†å® ç‰©åº—ï¼Œé—®è€æ¿ï¼šâ€œè¿™åªç‹—å¤šå°‘é’±ï¼Ÿâ€è€æ¿å›ç­”ï¼šâ€œè¿™æ˜¯ Wirehaired Pointing Griffonï¼Œå®ƒéœ€è¦ä¸€ç‚¹æ—¶é—´æ¥é€‚åº”æ–°ç¯å¢ƒï¼Œæ‰€ä»¥ä»·æ ¼æ˜¯å…¶ä»–çš„ç‹—çš„ä¸¤å€ã€‚â€é‚£ä¸ªäººé—®ï¼šâ€œä¸ºä»€ä¹ˆè¦è¿™ä¹ˆè´µï¼Ÿâ€è€æ¿å›ç­”ï¼šâ€œä½ æ²¡çœ‹è§å®ƒåé¢çš„é‚£ä¸ªç‰Œå­å—ï¼Ÿå†™ç€â€˜å‹æƒ…æé†’ï¼šä¸»äººæ˜¯ä¸ªçå­ã€‚â€™â€',\n",
       " 'ä¸€ä¸ªäººé—®ï¼šâ€œçŒªæ˜¯æ€ä¹ˆä¸Šæ ‘çš„ï¼Ÿâ€ å¦ä¸€ä¸ªäººå›ç­”ï¼šâ€œå˜æˆçƒ¤çŒªè¹„å°±ä¸Šå»äº†ã€‚â€\\\\n\\\\nï¼ˆæ³¨ï¼šè¿™ä¸ªç¬‘è¯æ˜¯åœ¨è°ƒä¾ƒçŒªä¸€èˆ¬æ¥è¯´ä¸ä¼šä¸Šæ ‘ï¼Œä½†ç»è¿‡çƒ¹é¥ªåŠ å·¥åçš„çƒ¤çŒªè¹„å´å¯ä»¥æŒ‚åœ¨æ ‘ä¸Šã€‚ï¼‰']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([\"çŒ«\",\"ç‹—\",\"çŒª\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelize steps\n",
    "\n",
    "RunnableParallel (aka. RunnableMap) makes it easy to execute multiple Runnables in parallel, and to return the output of these Runnables as a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': \"Why don't scientists trust atoms?\\\\n\\\\nBecause they make up everything!\",\n",
       " 'poem': 'Python, sleek and divine,\\\\nSlithering through codeine.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joke_chain = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\n",
    "poem_chain = (\n",
    "    ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\") | model\n",
    ")\n",
    "\n",
    "from langchain_core.runnables.base import RunnableParallel\n",
    "map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)\n",
    "\n",
    "map_chain.invoke({\"topic\": \"python\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
