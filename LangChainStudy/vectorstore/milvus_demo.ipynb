{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载知识库数据\n",
    "\n",
    "首先，需要使用标准格式加载数据。也就是说，我们需要将文本切成小块，从而确保传入 LLM 模型的数据为一段段小的文本片段。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "model_config_path = os.path.abspath(os.path.join('../custom_llms/'))\n",
    "sys.path.insert(0, module_path)\n",
    "sys.path.insert(0, model_config_path)\n",
    "\n",
    "from custom_llms import (\n",
    "    ZhipuAIEmbeddings,\n",
    "    Zhipuai_LLM,\n",
    "    load_api\n",
    ")\n",
    "api_key = load_api()\n",
    "model = Zhipuai_LLM(zhipuai_api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Milvus\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('../../docs/state_of_the_union.txt')\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们需要将小块的文本片段转化为向量并存储在向量数据库中。以下示例代码使用 OpenAI 的模型和 Zilliz Cloud 向量数据库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = ZhipuAIEmbeddings()\n",
    "\n",
    "vector_db = Milvus.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    connection_args={\n",
    "        \"host\": \"172.23.115.108\",\n",
    "        \"port\": \"19530\",\n",
    "        \"user\": \"\",\n",
    "        \"password\": \"\",\n",
    "        \"secure\": False\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查询数据\n",
    "\n",
    "加载数据后，可以在问答链（Chain）中使用这些数据，下述代码主要解决上文提到的“幻觉”问题。\n",
    "\n",
    "使用similarity_search方法将查询语句转化为特征向量，然后在 Zilliz Cloud 中搜索相似向量，以及相关的文档内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = vector_db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行 load_qa_chain获取最终答案。这是一个最通用的用于回答问题的接口，它加载一整个链，可以根据所有数据库中文本进行问答。以下示例代码使用 OpenAI 作为 LLM 模型。在运行时，QA Chain 接收input_documents和 question，将其作为输入。input_documents是与数据库中的query相关的文档。LLM 基于这些文档的内容和所提问的问题来组织答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The president said that Ketanji Brown Jackson is one of the nation's top legal minds and will continue Justice Breyer's legacy of excellence. He nominated her to serve on the United States Supreme Court.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "llm = model\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
